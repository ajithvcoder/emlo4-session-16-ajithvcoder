apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "torchserve-sd3"
spec:
  predictor:
    serviceAccountName: s3-read-only
    pytorch:
      protocolVersion: v1
      storageUri: "s3://mybucket-emlo-mumbai/sd_small/"
      image: pytorch/torchserve-kfs:0.12.0-gpu
      resources:
        limits:
          cpu: "8"
          memory: 16Gi
          nvidia.com/gpu: "1"
      # resources:
      #   limits:
      #     cpu: "16"
      #     memory: 32Gi
      #     nvidia.com/gpu: "1"
      # resources:
      #   limits:
      #     cpu: "8"
      #     memory: "16Gi"
      #     nvidia.com/gpu: "1"
      #   requests:
      #     cpu: "7"
      #     memory: "14Gi"
      #     nvidia.com/gpu: "1"
      env:
        - name: TS_DISABLE_TOKEN_AUTHORIZATION
          value: "true"
        # # Model loading parameters
        - name: MODEL_LOAD_MAX_TRY
          value: "20"  # Increase from default 10
        - name: MODEL_LOAD_DELAY
          value: "60"  # Increase from default 30 seconds
        - name: MODEL_LOAD_TIMEOUT
          value: "120"  # Increase from default 5 seconds
        # - name: MODEL_LOAD_CUSTOMIZED
        #   value: "false"
        # Additional TorchServe parameters
        - name: TS_RESPONSE_TIMEOUT
          value: "1200"  # 20 minutes
        # - name: TS_DEFAULT_WORKERS_PER_MODEL
        #   value: "1" 